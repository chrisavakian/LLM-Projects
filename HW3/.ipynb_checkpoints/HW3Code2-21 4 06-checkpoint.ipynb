{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18376a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "a = np.dstack((x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e990181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 6.1\n",
    "b = np.array([1, 1])\n",
    "c = np.array([[1, 0], [0, 2]])\n",
    "\n",
    "randvar = multivariate_normal(b, c)\n",
    "zed = randvar.pdf(a)\n",
    "\n",
    "plot.figure(figsize=(6,6))\n",
    "plot.contourf(x, y, zed)\n",
    "plot.title('6.1')\n",
    "plot.xlabel('X')\n",
    "plot.ylabel('Y')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f5a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 6.2\n",
    "b = np.array([-1, 2])\n",
    "c = np.array([[2, 1], [1, 4]])\n",
    "\n",
    "randvar = multivariate_normal(b, c)\n",
    "zed = randvar.pdf(a)\n",
    "\n",
    "plot.figure(figsize=(6,6))\n",
    "plot.contourf(x, y, zed)\n",
    "plot.title('6.2')\n",
    "plot.xlabel('X')\n",
    "plot.ylabel('Y')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95169c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 6.3\n",
    "b1 = np.array([0, 2])\n",
    "b2 = np.array([2, 0])\n",
    "c1 = c2 = np.array([[2, 1], [1, 1]])\n",
    "\n",
    "randvar1 = multivariate_normal(b1, c1)\n",
    "randvar2 = multivariate_normal(b2, c2)\n",
    "zed = randvar1.pdf(a) - randvar2.pdf(a)\n",
    "\n",
    "plot.figure(figsize=(6,6))\n",
    "plot.contourf(x, y, zed)\n",
    "plot.title('6.3')\n",
    "plot.xlabel('X')\n",
    "plot.ylabel('Y')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d272ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 6.4\n",
    "c2 = np.array([[2, 1], [1, 4]])\n",
    "\n",
    "randvar2 = multivariate_normal(b2, c2)\n",
    "zed = randvar1.pdf(a) - randvar2.pdf(a)\n",
    "\n",
    "plot.figure(figsize=(6,6))\n",
    "plot.contourf(x, y, zed)\n",
    "plot.title('6.4')\n",
    "plot.xlabel('X')\n",
    "plot.ylabel('Y')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8913ef2c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Question 6.5\n",
    "b1 = np.array([1, 1])\n",
    "b2 = np.array([-1, -1])\n",
    "c1 = np.array([[2, 0], [0, 1]])\n",
    "c2 = np.array([[2, 1], [1, 2]])\n",
    "\n",
    "randvar1 = multivariate_normal(b1, c1)\n",
    "randvar2 = multivariate_normal(b2, c2)\n",
    "zed = randvar1.pdf(a) - randvar2.pdf(a)\n",
    "\n",
    "plot.figure(figsize=(6,6))\n",
    "plot.contourf(x, y, zed)\n",
    "plot.title('6.5')\n",
    "plot.xlabel('X')\n",
    "plot.ylabel('Y')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6b0eec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Question 7\n",
    "np.random.seed(0)\n",
    "\n",
    "n = 100\n",
    "X1_mu = 3\n",
    "X1_sigma = 3\n",
    "X2_mu = 4\n",
    "X2_sigma = 2\n",
    "a = 0.5\n",
    "\n",
    "X1 = np.random.normal(X1_mu, X1_sigma, n)\n",
    "print(\"X1:\\n\", X1, \"\\n\\n\")\n",
    "X2 = a * X1 + np.random.normal(X2_mu, X2_sigma, n)\n",
    "print(\"X2:\\n\", X2, \"\\n\\n\")\n",
    "X = np.stack((X1, X2), axis=1)\n",
    "print(\"X:\\n\", X, \"\\n\\n\")\n",
    "\n",
    "print(\"X[:,0]:\\n\", X[:,0], \"\\n\\n\")\n",
    "print(\"X[:,1]:\\n\", X[:,1], \"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "mean = np.mean(X, axis=0)\n",
    "print(\"Mean:\\n\", mean, \"\\n\")\n",
    "\n",
    "cov = np.cov(X, rowvar=False)\n",
    "print(\"Covariance:\\n\", cov, \"\\n\")\n",
    "\n",
    "evals, evecs = np.linalg.eig(cov)\n",
    "print(\"Eigenvalues:\\n\", evals, \"\\n\")\n",
    "print(\"Eigenvectors:\\n\", evecs, \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "plot.figure(figsize=(5,5))\n",
    "plot.scatter(X[:,0], X[:,1], label='Data points')\n",
    "\n",
    "plot.quiver(mean[0], mean[1], eigvecs[0,0], eigvecs[1,0], color='red', scale=eigvals[0], label='Evec1')\n",
    "plot.quiver(mean[0], mean[1], eigvecs[0,1], eigvecs[1,1], color='green', scale=eigvals[1], label='Evec2')\n",
    "\n",
    "plot.xlim(-15, 15)\n",
    "plot.ylim(-15, 15)\n",
    "\n",
    "plot.xlabel('X1')\n",
    "plot.ylabel('X2')\n",
    "\n",
    "plot.title('7.4')\n",
    "\n",
    "plot.legend()\n",
    "plot.show()\n",
    "\n",
    "\n",
    "\n",
    "X_rot = evecs.T @ (X - mean).T\n",
    "X_rot = X_rot.T\n",
    "\n",
    "plot.figure(figsize=(5,5))\n",
    "plot.scatter(X_rot[:,0], X_rot[:,1])\n",
    "\n",
    "plot.xlim(-15, 15)\n",
    "plot.ylim(-15, 15)\n",
    "\n",
    "plot.xlabel('X1_rot')\n",
    "plot.ylabel('X2_rot')\n",
    "\n",
    "plot.title('7.5')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef36c1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Question 8\n",
    "data_npz = np.load('data/mnist-data-hw3.npz')\n",
    "train_data = data_npz['training_data']\n",
    "train_labels = data_npz['training_labels']\n",
    "\n",
    "mean_dict = {}\n",
    "covariance_dict = {}\n",
    "\n",
    "for label in range(10):\n",
    "    samples = train_data[train_labels == label]\n",
    "    \n",
    "    l2_norms = np.linalg.norm(samples, axis=1)\n",
    "    l2_norms += 0.0001\n",
    "    samples = samples / l2_norms[:, np.newaxis]\n",
    "    \n",
    "    samples = samples.reshape(samples.shape[0], -1)\n",
    "    \n",
    "    mean_dict[label] = np.mean(samples, axis=0)\n",
    "    covariance_dict[label] = np.cov(samples, rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6c70e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "covariance_matrix = covariance_dict[0]\n",
    "\n",
    "# Visualize the covariance matrix\n",
    "plot.figure(figsize=(5, 5))\n",
    "plot.imshow(covariance_matrix, cmap='hot', interpolation='nearest')\n",
    "plot.title('8.2 Covariance Matrix HeatMap for digit 0')\n",
    "plot.colorbar(label='Covariance')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db752229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 8 LDA Working\n",
    "data_npz = np.load('data/mnist-data-hw3.npz')\n",
    "train_data = data_npz['training_data']\n",
    "train_labels = data_npz['training_labels']\n",
    "\n",
    "mean_dict = {}\n",
    "covariance_dict = {}\n",
    "\n",
    "for label in range(10):\n",
    "    samples = train_data[train_labels == label]\n",
    "    \n",
    "    l2_norms = np.linalg.norm(samples, axis=1)\n",
    "    l2_norms += 0.0001\n",
    "    samples = samples / l2_norms[:, np.newaxis]\n",
    "    \n",
    "    samples = samples.reshape(samples.shape[0], -1)\n",
    "    \n",
    "    mean_dict[label] = np.mean(samples, axis=0)\n",
    "    covariance_dict[label] = np.cov(samples, rowvar=False)\n",
    "    \n",
    "    \n",
    "\n",
    "mean_mat = np.array(list(mean_dict.values())).T\n",
    "\n",
    "pooled_covariance = np.mean(list(covariance_dict.values()), axis=0)\n",
    "\n",
    "epsilon = 1e-5\n",
    "pooled_covariance += epsilon * np.eye(pooled_covariance.shape[0])\n",
    "\n",
    "inv_pooled_covariance = np.linalg.inv(pooled_covariance)\n",
    "\n",
    "class_priors = np.array([np.mean(train_labels == label) for label in range(10)])\n",
    "\n",
    "def lda(validation_data, means_matrix, inv_pooled_covariance, class_priors):\n",
    "    flat_validation_data = validation_data.reshape(validation_data.shape[0], -1).T\n",
    "    discriminant_values = np.zeros((10, validation_data.shape[0]))\n",
    "    \n",
    "    for i, mean_vector in enumerate(means_matrix.T):\n",
    "        flat_mean_vector = mean_vector.ravel()\n",
    "        term1 = np.dot(inv_pooled_covariance, flat_mean_vector)\n",
    "        term2 = 0.5 * np.dot(flat_mean_vector.T, term1)\n",
    "        discriminant_values[i, :] = np.dot(term1.T, flat_validation_data) - term2 + np.log(class_priors[i])\n",
    "    \n",
    "    predictions = np.argmax(discriminant_values, axis=0)\n",
    "    return predictions\n",
    "\n",
    "np.random.seed(0)\n",
    "validation_indices = np.random.choice(len(train_data), size=10000, replace=False)\n",
    "validation_data = train_data[validation_indices]\n",
    "validation_labels = train_labels[validation_indices]\n",
    "\n",
    "validation_predictions = lda(validation_data, mean_mat, inv_pooled_covariance, class_priors)\n",
    "\n",
    "error_rate = 1 - np.sum(validation_predictions == validation_labels) / len(validation_labels)\n",
    "\n",
    "training_sizes = [100, 200, 500, 1000, 2000, 5000, 10000, 30000, 50000]\n",
    "error_rates = []\n",
    "\n",
    "for size in training_sizes:\n",
    "    subset_indices = np.random.choice(len(train_data), size=size, replace=False)\n",
    "    subset_data = train_data[subset_indices]\n",
    "    subset_labels = train_labels[subset_indices]\n",
    "    \n",
    "    subset_means_matrix = np.array([np.mean(subset_data[subset_labels == label], axis=0) for label in range(10)]).T\n",
    "    \n",
    "    subset_covariances = []\n",
    "    for label in range(10):\n",
    "        label_data = subset_data[subset_labels == label]\n",
    "        if label_data.ndim == 2 and label_data.shape[0] > 1:\n",
    "            subset_covariances.append(np.cov(label_data, rowvar=False))\n",
    "        else:\n",
    "            subset_covariances.append(pooled_covariance)\n",
    "    \n",
    "    subset_covariances = np.array(subset_covariances)\n",
    "    \n",
    "    subset_pooled_covariance = np.mean(subset_covariances, axis=0) + epsilon * np.eye(subset_covariances.shape[1])\n",
    "    subset_inv_pooled_covariance = np.linalg.inv(subset_pooled_covariance)\n",
    "    \n",
    "    subset_validation_predictions = lda(validation_data, subset_means_matrix, subset_inv_pooled_covariance, class_priors)\n",
    "    \n",
    "    subset_error_rate = 1 - np.sum(subset_validation_predictions == validation_labels) / len(validation_labels)\n",
    "    error_rates.append(subset_error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ba1644",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Validation Error Rate: {error_rate}')\n",
    "plt.plot(training_sizes, error_rates)\n",
    "plt.xlabel('Training Points')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.title('LDA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff4068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 8 QDA Working\n",
    "# Initialize an array to hold the inverse covariance matrices for each class\n",
    "data_npz = np.load('data/mnist-data-hw3.npz')\n",
    "train_data = data_npz['training_data']\n",
    "train_labels = data_npz['training_labels']\n",
    "\n",
    "mean_dict = {}\n",
    "covariance_dict = {}\n",
    "\n",
    "for label in range(10):\n",
    "    samples = train_data[train_labels == label]\n",
    "    \n",
    "    l2_norms = np.linalg.norm(samples, axis=1)\n",
    "    l2_norms += 0.0001\n",
    "    samples = samples / l2_norms[:, np.newaxis]\n",
    "    \n",
    "    samples = samples.reshape(samples.shape[0], -1)\n",
    "    \n",
    "    mean_dict[label] = np.mean(samples, axis=0)\n",
    "    covariance_dict[label] = np.cov(samples, rowvar=False)\n",
    "\n",
    "\n",
    "inv_covariances = {}\n",
    "epsilon = 1e-10\n",
    "\n",
    "# Compute the inverse of each covariance matrix\n",
    "for label in range(10):\n",
    "    # Regularize the covariance matrix to ensure it's invertible\n",
    "    cov = covariances[label] + epsilon * np.eye(covariances[label].shape[0])\n",
    "    \n",
    "    # Compute the inverse covariance matrix\n",
    "    inv_covariances[label] = np.linalg.inv(cov)\n",
    "\n",
    "def qda_classify_all(validation_data, means_matrix, inv_covariances, class_priors):\n",
    "    # Flatten and transpose validation data for vectorized operations\n",
    "    flat_validation_data = validation_data.reshape(validation_data.shape[0], -1).T\n",
    "    \n",
    "    # Initialize an array to hold the discriminant values for each class\n",
    "    discriminant_values = np.zeros((10, validation_data.shape[0]))\n",
    "    \n",
    "    # Compute discriminant values for each class\n",
    "    for i, mean_vector in enumerate(means_matrix.T):\n",
    "        # Flatten the mean vector\n",
    "        flat_mean_vector = mean_vector.ravel()\n",
    "        \n",
    "        # Compute the difference between the validation data and the mean\n",
    "        diff = flat_validation_data - flat_mean_vector[:, np.newaxis]\n",
    "        \n",
    "        # Compute the discriminant value for this class\n",
    "        term1 = -0.5 * np.sum(np.dot(diff.T, inv_covariances[i]) * diff.T, axis=1)\n",
    "        _, logdet = np.linalg.slogdet(inv_covariances[i])\n",
    "        term2 = -0.5 * logdet + np.log(class_priors[i])\n",
    "        discriminant_values[i, :] = term1 + term2\n",
    "    \n",
    "    # Predict the class with the highest discriminant value for each image\n",
    "    predictions = np.argmax(discriminant_values, axis=0)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Classify the validation set using the vectorized QDA function\n",
    "validation_predictions = qda_classify_all(validation_data, means_matrix, inv_covariances, class_priors)\n",
    "\n",
    "# Compute the error rate on the validation set\n",
    "error_rate = 1 - np.sum(validation_predictions == validation_labels) / len(validation_labels)\n",
    "\n",
    "# Repeat the same tests and error rate calculations you did for LDA\n",
    "error_rates = []\n",
    "\n",
    "for size in training_sizes:\n",
    "    # Train QDA model on a subset of the training data\n",
    "    subset_indices = np.random.choice(len(training_data), size=size, replace=False)\n",
    "    subset_data = training_data[subset_indices]\n",
    "    subset_labels = training_labels[subset_indices]\n",
    "    \n",
    "    # Compute means for the subset\n",
    "    subset_means_matrix = np.array([np.mean(subset_data[subset_labels == label], axis=0) for label in range(10)]).T\n",
    "    \n",
    "    # Compute covariances for the subset\n",
    "    subset_covariances = []\n",
    "    for label in range(10):\n",
    "        label_data = subset_data[subset_labels == label]\n",
    "        if label_data.ndim == 2 and label_data.shape[0] > 1:  # Ensure there's more than one observation and data is two-dimensional\n",
    "            subset_covariances.append(np.cov(label_data, rowvar=False))\n",
    "        else:  # Handle the case with one or zero observations\n",
    "            # Use the overall pooled covariance as a placeholder\n",
    "            subset_covariances.append(pooled_covariance)\n",
    "    \n",
    "    subset_covariances = np.array(subset_covariances)\n",
    "    \n",
    "    # Compute the regularized inverse covariance matrices for the subset\n",
    "    subset_inv_covariances = [np.linalg.inv(cov + epsilon * np.eye(cov.shape[0])) for cov in subset_covariances]\n",
    "    \n",
    "    # Classify validation set using the trained QDA model\n",
    "    subset_validation_predictions = qda_classify_all(validation_data, subset_means_matrix, subset_inv_covariances, class_priors)\n",
    "    \n",
    "    # Compute error rate and append to the list\n",
    "    subset_error_rate = 1 - np.sum(subset_validation_predictions == validation_labels) / len(validation_labels)\n",
    "    error_rates.append(subset_error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f0498",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Validation Error Rate: {error_rate}')\n",
    "plt.plot(training_sizes, error_rates, marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Number of Training Points')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.title('Error Rate vs Number of Training Points (QDA)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534d9e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 8.3d\n",
    "training_sizes = [100, 200, 500, 1000, 2000, 5000, 10000, 30000, 50000]\n",
    "error_rates = []\n",
    "\n",
    "# Initialize a dictionary to hold the error rates for each digit\n",
    "error_rates_per_digit = {i: [] for i in range(10)}\n",
    "\n",
    "for size in training_sizes:\n",
    "    # Train QDA model on a subset of the training data\n",
    "    subset_indices = np.random.choice(len(training_data), size=size, replace=False)\n",
    "    subset_data = training_data[subset_indices]\n",
    "    subset_labels = training_labels[subset_indices]\n",
    "    \n",
    "    # Compute means for the subset\n",
    "    subset_means_matrix = np.array([np.mean(subset_data[subset_labels == label], axis=0) for label in range(10)]).T\n",
    "    \n",
    "    # Compute covariances for the subset\n",
    "    subset_covariances = []\n",
    "    for label in range(10):\n",
    "        label_data = subset_data[subset_labels == label]\n",
    "        if label_data.ndim == 2 and label_data.shape[0] > 1:  # Ensure there's more than one observation and data is two-dimensional\n",
    "            subset_covariances.append(np.cov(label_data, rowvar=False))\n",
    "        else:  # Handle the case with one or zero observations\n",
    "            # Use the overall pooled covariance as a placeholder\n",
    "            subset_covariances.append(pooled_covariance)\n",
    "    \n",
    "    subset_covariances = np.array(subset_covariances)\n",
    "    \n",
    "    # Compute the regularized inverse covariance matrices for the subset\n",
    "    subset_inv_covariances = [np.linalg.inv(cov + epsilon * np.eye(cov.shape[0])) for cov in subset_covariances]\n",
    "    \n",
    "    # Classify validation set using the trained QDA model\n",
    "    subset_validation_predictions = qda_classify_all(validation_data, subset_means_matrix, subset_inv_covariances, class_priors)\n",
    "    \n",
    "    # Compute error rate for each digit and append to the list\n",
    "    for i in range(10):\n",
    "        digit_indices = np.where(validation_labels == i)\n",
    "        digit_predictions = subset_validation_predictions[digit_indices]\n",
    "        digit_labels = validation_labels[digit_indices]\n",
    "        digit_error_rate = 1 - np.sum(digit_predictions == digit_labels) / len(digit_labels)\n",
    "        error_rates_per_digit[i].append(digit_error_rate)\n",
    "\n",
    "# Plot the error rates for each digit\n",
    "for i in range(10):\n",
    "    plt.plot(training_sizes, error_rates_per_digit[i], marker='o', label=f'Digit {i}')\n",
    "\n",
    "plt.xlabel('Training Points')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.title('8.3d')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec04bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc59dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b2a9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
